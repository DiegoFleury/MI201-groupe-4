{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BERT Architecture Explained\n",
        "## MI201\n",
        "\n",
        "##**Group 4** :\n",
        "- Diego FLEURY CORRÃŠA DE MORAES\n",
        "- Hazael SOLEDADE DE ARAUJO JUMONJI\n",
        "- Lucas DE OLIVEIRA MARTIM\n",
        "\n",
        "### Project 3 : **Sentiment Analysis Using LLMs**"
      ],
      "metadata": {
        "id": "eir4v2rZCnu3"
      },
      "id": "eir4v2rZCnu3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "\n",
        "In this notebook, we aim to answer questions 5 and 6 in more detail, providing the necessary context without overwhelming the main experiments notebook with too many cells. Transformer-based architectures have revolutionized NLP, so it's important to explain them properly. To keep the focus on theory while minimizing code, we've opted for this separate, less code-intensive notebook.\n",
        "\n",
        "In addition to reading the [paper](https://arxiv.org/pdf/1810.04805), we followed it with a [visual intuition](https://www.youtube.com/watch?v=wjZofJX0v4M&pp=ygUJM2IxYiBMTG1z) of the general picture of LLMs and, lastly, an excelent [series of blog post](https://jalammar.github.io/)."
      ],
      "metadata": {
        "id": "HXtJLztFxsGF"
      },
      "id": "HXtJLztFxsGF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE**: The images from this notebook are taken from a series of Jay Alammar's blog posts. They're often used in media for explanation purposes, and are licensed under [Creative Commons](http://creativecommons.org/licenses/by-nc-sa/4.0/). Here are the references:\n",
        "\n",
        "\n",
        "- *Alammar, J* (2018). **The Illustrated Transformer** [Blog post].\n",
        "  * Retrieved from https://jalammar.github.io/illustrated-transformer/\n",
        "\n",
        "-*Alammar, J* (2018). **Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)** [Blog post].\n",
        "  * Retrieved from https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\n",
        "\n",
        "- *Alammar, J* (2018). **The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)** [Blog post].\n",
        "  * Retrieved from https://jalammar.github.io/illustrated-bert/\n",
        "\n",
        "-*Alammar, J* (2019). **A Visual Guide to Using BERT for the First Time** [Blog post].\n",
        "  * Retrieved from https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n"
      ],
      "metadata": {
        "id": "Be7ddQYc_FZh"
      },
      "id": "Be7ddQYc_FZh"
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML # Needed for media"
      ],
      "metadata": {
        "id": "D_j3HK2qCo_9"
      },
      "id": "D_j3HK2qCo_9",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Attention is All You Need\n",
        "\n",
        "The **\"Attention Is All You Need\"** [paper](https://arxiv.org/pdf/1706.03762) introduced the Transformer, a model that revolutionized Natural Language Processing (NLP). Unlike previous architectures that relied on recurrence (Recurent Neural Networks), the Transformer's context-capturing mechanism is built entirely on self-attention mechanisms, allowing for faster training and better handling of long-range dependencies. This innovation laid the foundation for modern models like BERT and GPT, making attention-based architectures the new standard in deep learning for NLP."
      ],
      "metadata": {
        "id": "_jAVKfZcxyMk"
      },
      "id": "_jAVKfZcxyMk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Natural Language Processing\n",
        "\n",
        "**But what is NLP ?**\n",
        "\n",
        "NLP stands for Natural Language Processing, and is a field of artificial intelligence that enables machines to understand, interpret, and generate human language. It combines linguistics and machine learning to power many real-world applications.\n",
        "\n"
      ],
      "metadata": {
        "id": "Nzy39Kg-yJ67"
      },
      "id": "Nzy39Kg-yJ67"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings (Classical vs Contextual)\n",
        "\n",
        "However, a fundamental challenge in NLP is that computers **cannot** process *raw* text **directly** - they operate on **numerical** data. This means that any text input must first be transformed into a numerical representation. Early approaches relied on simple techniques like **one-hot encoding** or **TF-IDF**, but these methods failed to capture more nuanced **semantically**-rich relationships between words. To address this, more advanced techniques called **embeddings** were developed, mapping words to **dense** (not one-hot-encoded) vector representations that encode semantic meaning. The various directions in the constructed vector space can, therefore, encode some form of meaning in their directions.\n",
        "\n",
        "Even so, traditional (non-neural) embeddings like **Word2Vec** or **GloVe** assign a **fixed** vector to each word, ignoring context (for example, \"stick\" has a different meaning in the sentence \"stick to it\" than in \"my dog found a nice stick in the park\"). This limitation led to the development of **contextual** embeddings, where the representation of a word **dynamically** changes based on its surrounding words - an approach central to Transformer-based models like BERT."
      ],
      "metadata": {
        "id": "1c_rR6HEyroi"
      },
      "id": "1c_rR6HEyroi"
    },
    {
      "cell_type": "code",
      "source": [
        "contextEmbeddingURL = \"https://jalammar.github.io/images/context.png\"\n",
        "HTML(f\"<img src='{contextEmbeddingURL}' width='800' height='300' controls></img>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "kYFtUHP3L_3q",
        "outputId": "375feb06-f2b4-451e-9f3d-1112cc2f6dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src='https://jalammar.github.io/images/context.png' width='800' height='300' controls></img>"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "id": "kYFtUHP3L_3q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What are sequence to sequence and language models\n",
        "\n",
        "One of the most important distinctions to be made in the field is the task's nature. Be it on it's **structure** or **objective**.\n",
        "\n",
        "Practical applications may involve generating a **single** output from a **sequence** of text (*sequence-to-one*, e.g., sentiment classification), a **sequence** from a **single** input (one-to-sequence, e.g., text generation from a prompt) or a multitude of **text** from some previous **text** (*sequence-to-sequence*, e.g., machine translation)."
      ],
      "metadata": {
        "id": "TLlF04l2zG1a"
      },
      "id": "TLlF04l2zG1a"
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seqURL= \"https://jalammar.github.io/images/seq2seq_1.mp4\"\n",
        "HTML(f\"<video src='{seq2seqURL}' autoplay loop width='800' height='600' controls></video>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "DUB6Yoo9CfoE",
        "outputId": "5e69d7ee-8d00-4e46-f9fa-ea233a0e3536"
      },
      "id": "DUB6Yoo9CfoE",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video src='https://jalammar.github.io/images/seq2seq_1.mp4' autoplay loop width='800' height='600' controls></video>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Language models** are a specific type of NLP model that concerns itself with the task of predicting the next word in a sequence of text, given the previous/current sequence (that contextualizes it). It's implied in the objective's nature that the model should capture the probability distribution of words and phrases, capturing implicit **semantic** relationships. However, while this process helps model linguistic structure, it does not equate to true comprehension - this falls under the domain of **Natural Language Understanding** (NLU).  \n",
        "\n",
        "\n",
        "The \"*Attention is All You Need*\" paper introduced the transformer architecture as a **language model** for a **sequence-to-sequence** task: **neural machine translation**, and we'll briefly go into detail here, as it's crucial for the comprehension of the BERT paper."
      ],
      "metadata": {
        "id": "IFdv_Gu1tWLj"
      },
      "id": "IFdv_Gu1tWLj"
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_enc_nmt_decURL = \"https://jalammar.github.io/images/seq2seq_4.mp4\"\n",
        "HTML(f\"<video src='{seq2seq_enc_nmt_decURL}' autoplay loop width='800' height='600' controls></video>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "A1Wxogp7C-e2",
        "outputId": "2d6aad26-e64d-4c17-bc09-dbc7b00a12c1"
      },
      "id": "A1Wxogp7C-e2",
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video src='https://jalammar.github.io/images/seq2seq_4.mp4' autoplay loop width='800' height='600' controls></video>"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-BERT : Recurrent Neural Networks"
      ],
      "metadata": {
        "id": "C1ZTYQV6ySc8"
      },
      "id": "C1ZTYQV6ySc8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first see what were the previous solutions to the problem, prior to **BERT**.\n",
        "\n",
        "Contextual embeddings using neural networks were steadily advancing, with **Recurrent Neural Networks** (RNNs) being the primary tool of choice. Their inherently **sequential** nature made them a natural choice for modelling the nuances of text processing, as they process input word by word, maintaining a notion of order and context. This resolved theoretically problem of conditioning on context the meaning of each word.\n",
        "\n",
        "(Explain briefly the idea of passing hidden states between cells, and backprop throgh time)\n",
        "\n",
        "The core mechanism of RNNs is the **hidden state** $ h_t $, which acts as a memory that carries **information** from previous words in a sequence. At each time step $t$, the RNN updates its hidden state as follows:\n",
        "\n",
        "$h_t = f(W_h h_{t-1} + W_x x_t)$\n",
        "\n",
        "where:\n",
        "- $x_t$ is the current input word,\n",
        "- $h_{t-1}$ is the hidden state from the previous time step,\n",
        "- $W_h$ and $W_x$ are weight matrices,\n",
        "- $f$ is a non-linear activation function (typically tanh or ReLU).\n",
        "\n",
        "This allows information to be passed from **word to word**, enabling the model to capture sequential dependencies. Training happens through **Backpropagation Through Time** (BPTT), where gradients are propagated backward across all time steps to adjust the model's parameters.\n",
        "\n",
        "While in theory this mechanism enables learning long-range dependencies, in practice, RNNs struggle with it due to the vanishing gradient problem - gradients become too small during backpropagation, preventing effective learning of distant relationships. Even architectures designed to mitigate this, such as **Long Short-Term Memory** (LSTM) and **Gated Recurrent Units** (GRU), still face limitations when dealing with very long sequences.\n",
        "\n",
        "These challenges paved the way for **Transformers**, which fully replace *recurrence* with **attention**, enabling **parallel** processing and eliminating the constraints of sequential modelsâ€”leading to architectures like BERT, which efficiently leverage computational power to capture context."
      ],
      "metadata": {
        "id": "OB-R9Xo74V8h"
      },
      "id": "OB-R9Xo74V8h"
    },
    {
      "cell_type": "code",
      "source": [
        "RNN_explanationURL = \"https://jalammar.github.io/images/RNN_1.mp4\"\n",
        "HTML(f\"<video src='{RNN_explanationURL}' autoplay loop width='800' height='600' controls></video>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "wxoOZo9AMJli",
        "outputId": "091b5128-a738-4fa2-93d4-7e12dbe71708"
      },
      "id": "wxoOZo9AMJli",
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video src='https://jalammar.github.io/images/RNN_1.mp4' autoplay loop width='800' height='600' controls></video>"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice, in the model bellow, the time it would take to process sequence. It needs to flow through (in this example, of neural machine translation) lots of RNN cells before going into the decoder, and the bottleneck vector (the one between the encoder and decoder) needs to capture **ALL** of the original phrase semantic meaning, independent of it's initial length. A very difficult task, given that it needs to have always the same dimentions."
      ],
      "metadata": {
        "id": "9lMteEf69ePm"
      },
      "id": "9lMteEf69ePm"
    },
    {
      "cell_type": "code",
      "source": [
        "RNN_neural_machine_translationURL = \"https://jalammar.github.io/images/seq2seq_6.mp4\"\n",
        "HTML(f\"<video src='{RNN_neural_machine_translationURL}' autoplay loop width='800' height='600' controls></video>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "TimXN9ESMdxL",
        "outputId": "b744b375-6200-4358-c423-7fdf5c4befd8"
      },
      "id": "TimXN9ESMdxL",
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video src='https://jalammar.github.io/images/seq2seq_6.mp4' autoplay loop width='800' height='600' controls></video>"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Attention Mechanism\n",
        "\n",
        "Enough teasing ! What is then the attention mechanism ?\n",
        "\n",
        "The attention mechanism comes from two principles:\n",
        "\n",
        "- **Neural Networks benefit greatly from scale**, and large-scale performance cannot be achieved within reasonable time and cost constraints without **paralelization**. Powerful NLP models should, therefore, be **paralelizable**, not sequential.\n",
        "\n",
        "- Whatever mechanism these networks apply should have **constant-time access between each pair of words** in the internal data structure, and optimization should take into consideration that. This would allow the network to be able to capture the context independently of the lenght of the sequence.\n"
      ],
      "metadata": {
        "id": "V6ZaoNyAyeNj"
      },
      "id": "V6ZaoNyAyeNj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Core Idea of Attention**\n",
        "\n",
        "Instead of processing words one at a time like RNNs, the attention mechanism allows a model to **weigh** the importance of all words in a sentence **simultaneously** when interpreting a given word. This is done by computing attention **scores** between each word pair, creating a weighted representation of **context**.\n",
        "\n",
        "Mathematically, attention computes an output vector for each word as a weighted sum of all other words in the sentence. This weight is determined by how **relevant** each other word is to the current word, which is dynamically learned during training."
      ],
      "metadata": {
        "id": "yjJLgXTUC0GM"
      },
      "id": "yjJLgXTUC0GM"
    },
    {
      "cell_type": "code",
      "source": [
        "attentionURL = \"https://jalammar.github.io/images/t/transformer_self-attention_visualization.png\"\n",
        "HTML(f\"<img src='{attentionURL}' width='600' height='600' controls></img>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "uEgXNkquGMDl",
        "outputId": "1294e2e1-3366-49df-d7be-abe4771abdc4"
      },
      "id": "uEgXNkquGMDl",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src='https://jalammar.github.io/images/t/transformer_self-attention_visualization.png' width='600' height='600' controls></img>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective is to model a transformation in the embedding space: from the input vectors to a **richer contextual representation** at the end. All of this follows a nice [visual intuition](https://www.youtube.com/watch?v=wjZofJX0v4M&pp=ygUJM2IxYiBMTG1z).\n",
        "\n",
        "The idea is to let the words \"interact\" and the way this is implemented is by making a strong analogy with database concepts:\n",
        "\n",
        " - Each word **broadcasts** it's useful semantic meaning (*key*)\n",
        " - Each word **requests** relevant information from the rest of the words(*query*).\n",
        " - The manner of which these interactions affect the word's representation is captured through the **weighted** update of a vector (*value*) that modifies the position of the original embedding in the embedding space, such that it \"soakes in\" context from the surroundings, and moves it incrementally towards a more precise representation.\n",
        "\n",
        "All of this is constructed via the multiplication of the original vector by weight matrices for each component (**Q,K,V**)."
      ],
      "metadata": {
        "id": "QF4eno3UDCNc"
      },
      "id": "QF4eno3UDCNc"
    },
    {
      "cell_type": "code",
      "source": [
        "attention_computationURL = \"https://jalammar.github.io/images/t/self-attention-matrix-calculation.png\"\n",
        "HTML(f\"<img src='{attention_computationURL}' width='600' height='700' controls></img>\")"
      ],
      "metadata": {
        "id": "GPEBYGCtHRQ0",
        "outputId": "1f576dba-9312-4838-ef24-8e87dfd1fb8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src='https://jalammar.github.io/images/t/self-attention-matrix-calculation.png' width='600' height='700' controls></img>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "id": "GPEBYGCtHRQ0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This happens for all the words in the input."
      ],
      "metadata": {
        "id": "BE4_uCqiI5PR"
      },
      "id": "BE4_uCqiI5PR"
    },
    {
      "cell_type": "code",
      "source": [
        "attention_weightsURL = \"https://jalammar.github.io/images/t/transformer_self_attention_vectors.png\"\n",
        "HTML(f\"<img src='{attention_weightsURL}' width='900' height='600' controls></img>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "uq4XGk-NGwOl",
        "outputId": "a2559d6d-2b65-46ea-ee94-b03d0c237cc9"
      },
      "id": "uq4XGk-NGwOl",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src='https://jalammar.github.io/images/t/transformer_self_attention_vectors.png' width='900' height='600' controls></img>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For computing the weighting from the key-query interaction a softmax operation is used."
      ],
      "metadata": {
        "id": "AsX0QyPLI93Z"
      },
      "id": "AsX0QyPLI93Z"
    },
    {
      "cell_type": "code",
      "source": [
        "softmaxedURL = \"https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\"\n",
        "HTML(f\"<img src='{softmaxedURL}' width='900' height='400' controls></img>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "Fji3hAGqHm67",
        "outputId": "8bb99bd7-e472-46c7-b912-1b1a693e5c37"
      },
      "id": "Fji3hAGqHm67",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src='https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png' width='900' height='400' controls></img>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **K, Q, V representation** enables **parallelization** because all words are transformed into these vectors simultaneously, avoiding sequential dependencies. It also ensures **constant-time access** between words, as attention scores are computed in a single matrix operation, allowing each word to directly interact with all others regardless of sequence length."
      ],
      "metadata": {
        "id": "AOiYfT03Ko3I"
      },
      "id": "AOiYfT03Ko3I"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Transformer Architecture"
      ],
      "metadata": {
        "id": "dT0OMRLRy5eC"
      },
      "id": "dT0OMRLRy5eC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Transformer architecture was originally designed for **machine translation**, requiring two main components: an **encoder** (processing the source language) and a **decoder** (generating the target language). While both use attention mechanisms, they apply them differently based on their roles.  \n",
        "\n",
        "The **encoder** relies on **self-attention**, where each word attends to all others in the input sequence, capturing full contextual meaning. The **decoder**, however, performs **masked self-attention**, which means each word can only attend to **previous** words in the sequence. This is because the decoder is functioning as a **language model**, generating text **autoregressively** - predicting words one at a time, and feeding them back into it's input, without access to future tokens, ensuring valid sequential output.  \n",
        "\n",
        "Additionally, the decoder includes **cross-attention** (represented by the \"Encoder-Decoder Attention\" block in the image bellow), which allows it to attend to the encoder's output, effectively learning how to map the input sentence to its translated version."
      ],
      "metadata": {
        "id": "rT4-_v4CK_L0"
      },
      "id": "rT4-_v4CK_L0"
    },
    {
      "cell_type": "code",
      "source": [
        "transformerHoleURL = \"https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png\"\n",
        "HTML(f\"<img src='{transformerHoleURL}' width='1000' height='600' controls></img>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "NJJgkC_UKrdd",
        "outputId": "f54bf789-397c-438e-eeb2-388c289fb66e"
      },
      "id": "NJJgkC_UKrdd",
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src='https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png' width='1000' height='600' controls></img>"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-Head Self-Attention"
      ],
      "metadata": {
        "id": "G3x_FjzvgOwO"
      },
      "id": "G3x_FjzvgOwO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "One important sidenote: the transformer doesn't employ just a single set of attention weights for each layer, but lots of them, each of which called a \"attention head\". The idea is to allow each head to specialize in different types of word semantics."
      ],
      "metadata": {
        "id": "JVHwnpYwMsf6"
      },
      "id": "JVHwnpYwMsf6"
    },
    {
      "cell_type": "code",
      "source": [
        "multiheadURL = \"https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png\"\n",
        "HTML(f\"<img src='{multiheadURL}' width='900' height='600' controls></img>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "28sOMzdAID6K",
        "outputId": "616d2de1-5f87-4782-9f2c-fd33128538d4"
      },
      "id": "28sOMzdAID6K",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src='https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png' width='900' height='600' controls></img>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the end, all the outputs from the attention heads are concatenated."
      ],
      "metadata": {
        "id": "hZ5P39E8NIDJ"
      },
      "id": "hZ5P39E8NIDJ"
    },
    {
      "cell_type": "code",
      "source": [
        "multiheadConcatURL = \"https://jalammar.github.io/images/t/transformer_attention_heads_z.png\"\n",
        "HTML(f\"<img src='{multiheadConcatURL}' width='1000' height='600' controls></img>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "P04xaGDnIKGi",
        "outputId": "d1b6bf22-4162-4a59-b980-91213808a103"
      },
      "id": "P04xaGDnIKGi",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src='https://jalammar.github.io/images/t/transformer_attention_heads_z.png' width='1000' height='600' controls></img>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And a (learned) output-weight matrix returns the vectors to it's original size. Each head is another opportunity for paralelization."
      ],
      "metadata": {
        "id": "p89mDsgvNP5x"
      },
      "id": "p89mDsgvNP5x"
    },
    {
      "cell_type": "code",
      "source": [
        "transformerURL = \"https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png\"\n",
        "HTML(f\"<img src='{transformerURL}' width='1000' height='600' controls></img>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "sNhjh3QTIVD6",
        "outputId": "7796cc68-d9d9-4701-c7d9-8e8777479d1f"
      },
      "id": "sNhjh3QTIVD6",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src='https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png' width='1000' height='600' controls></img>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the end, the keys are passed to the decoder side of things, to perform cross-attention with the queries generated in the decoder. The values are also passed."
      ],
      "metadata": {
        "id": "SEj9cAcINlsf"
      },
      "id": "SEj9cAcINlsf"
    },
    {
      "cell_type": "code",
      "source": [
        "neural_machine_translationpt1URL = \"https://jalammar.github.io/images/t/transformer_decoding_1.gif\"\n",
        "HTML(f\"<img src='{neural_machine_translationpt1URL}' width='1000' height='600' controls></img>\")"
      ],
      "metadata": {
        "id": "oDfDIe6SLXFt",
        "outputId": "b980dffc-f918-4712-ab9c-ac06da3e681a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src='https://jalammar.github.io/images/t/transformer_decoding_1.gif' width='1000' height='600' controls></img>"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "id": "oDfDIe6SLXFt"
    },
    {
      "cell_type": "code",
      "source": [
        "neural_machine_translationpt2URL = \"https://jalammar.github.io/images/t/transformer_decoding_2.gif\"\n",
        "HTML(f\"<img src='{neural_machine_translationpt2URL}' width='1000' height='600' controls></img>\")"
      ],
      "metadata": {
        "id": "65TzlqlbLedE",
        "outputId": "a9ce0451-1419-4d08-83ec-b358e49a0ee6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src='https://jalammar.github.io/images/t/transformer_decoding_2.gif' width='1000' height='600' controls></img>"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "id": "65TzlqlbLedE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT"
      ],
      "metadata": {
        "id": "tCqq4ZLax9_s"
      },
      "id": "tCqq4ZLax9_s"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is BERT"
      ],
      "metadata": {
        "id": "chNJSvEwzOuh"
      },
      "id": "chNJSvEwzOuh"
    },
    {
      "cell_type": "code",
      "source": [
        "bert_motivationURL = \"https://jalammar.github.io/images/bert-transfer-learning.png\"\n",
        "HTML(f\"<img src='{bert_motivationURL}' width='800' height='400' controls></img>\")"
      ],
      "metadata": {
        "id": "BPzU09pedJHa",
        "outputId": "eb454b27-5afc-4b7e-dabe-4201f0573ebd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src='https://jalammar.github.io/images/bert-transfer-learning.png' width='800' height='400' controls></img>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "id": "BPzU09pedJHa"
    },
    {
      "cell_type": "code",
      "source": [
        "BERT_tasks = \"https://jalammar.github.io/images/bert-tasks.png\"\n",
        "HTML(f\"<img src='{BERT_tasks}' width='800' height='500' controls></img>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "RwISpAG_hIni",
        "outputId": "4d4c022c-f6a3-498e-b366-c98d8a737254"
      },
      "id": "RwISpAG_hIni",
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src='https://jalammar.github.io/images/bert-tasks.png' width='800' height='500' controls></img>"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Internal Architecture"
      ],
      "metadata": {
        "id": "HaMVuVoazeXA"
      },
      "id": "HaMVuVoazeXA"
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_archURL = \"https://jalammar.github.io/images/t/transformer_resideual_layer_norm.png\"\n",
        "HTML(f\"<img src='{encoder_archURL}' width='800' height='600' controls></img>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "6NujP2taKIou",
        "outputId": "f7a7a977-3533-43f5-ba80-03b74638d63f"
      },
      "id": "6NujP2taKIou",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src='https://jalammar.github.io/images/t/transformer_resideual_layer_norm.png' width='800' height='600' controls></img>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input (Tokens, embeddings, segment embeddings, positional encodings)"
      ],
      "metadata": {
        "id": "cN0a8rE4zgtp"
      },
      "id": "cN0a8rE4zgtp"
    },
    {
      "cell_type": "code",
      "source": [
        "positional_encodingURL = \"https://jalammar.github.io/images/t/transformer_positional_encoding_example.png\"\n",
        "HTML(f\"<img src='{positional_encodingURL}' width='1000' height='300' controls></img>\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "q97slRRaI67Z",
        "outputId": "263ae376-750a-4ff0-ddcd-a91c77077ee0"
      },
      "id": "q97slRRaI67Z",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src='https://jalammar.github.io/images/t/transformer_positional_encoding_example.png' width='1000' height='300' controls></img>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positional_encoding_plotURL = \"https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png\"\n",
        "HTML(f\"<img src='{positional_encoding_plotURL}' width='900' height='500' controls></img>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "j3uquNpJJdoJ",
        "outputId": "6753d9a9-b412-4a9e-df74-0aad0d3cb93a"
      },
      "id": "j3uquNpJJdoJ",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src='https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png' width='900' height='500' controls></img>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-Head Self-Attention"
      ],
      "metadata": {
        "id": "80eWew1Iz7A_"
      },
      "id": "80eWew1Iz7A_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalization, Skip Connections and Feed-Forward\n",
        "\n",
        "Layer norm instead of batch norm due to :\n",
        "\n",
        "- Better adaptation to sequence data\n",
        "- Ease of paralelization\n",
        "- Same computation in train & test time\n",
        "\n",
        "Skip connections:\n",
        "\n",
        "- Facilitate propagation of gradients in earlier layers\n",
        "- Numerically stabilize gradients to be closer to 1\n",
        "- Make attention learn features of it's own, without worrying about preserving the output (it's actually part of the semantics of the network)"
      ],
      "metadata": {
        "id": "zy1z9BGwz_JY"
      },
      "id": "zy1z9BGwz_JY"
    },
    {
      "cell_type": "code",
      "source": [
        "skipConnectionsURL = \"https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png\"\n",
        "HTML(f\"<img src='{skipConnectionsURL}' width='700' height='600' controls></img>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "Hm9XxzwuKVzG",
        "outputId": "80f37767-e0cb-4233-f9e5-a454b33536ef"
      },
      "id": "Hm9XxzwuKVzG",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src='https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png' width='700' height='600' controls></img>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT Pretraining"
      ],
      "metadata": {
        "id": "St4XQ7lRzR5Z"
      },
      "id": "St4XQ7lRzR5Z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Masked** Language Modeling"
      ],
      "metadata": {
        "id": "op8Qv3gT0GCn"
      },
      "id": "op8Qv3gT0GCn"
    },
    {
      "cell_type": "code",
      "source": [
        "MLM_URL = \"https://jalammar.github.io/images/BERT-language-modeling-masked-lm.png\"\n",
        "HTML(f\"<img src='{MLM_URL}' width='900' height='600' controls></img>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "G4kph_qVgnf0",
        "outputId": "4f897003-db7e-4787-c337-fe55dedb1f68"
      },
      "id": "G4kph_qVgnf0",
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src='https://jalammar.github.io/images/BERT-language-modeling-masked-lm.png' width='900' height='600' controls></img>"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NSP ([CLS] token)"
      ],
      "metadata": {
        "id": "OpdoBu5a0H0f"
      },
      "id": "OpdoBu5a0H0f"
    },
    {
      "cell_type": "code",
      "source": [
        "NSP_URL = \"https://jalammar.github.io/images/bert-next-sentence-prediction.png\"\n",
        "HTML(f\"<img src='{NSP_URL}' width='900' height='600' controls></img>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "KH9poLI8gwlr",
        "outputId": "42bc0db1-3a42-437b-b2dc-19a8808271cc"
      },
      "id": "KH9poLI8gwlr",
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src='https://jalammar.github.io/images/bert-next-sentence-prediction.png' width='900' height='600' controls></img>"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_classificationURL = \"https://jalammar.github.io/images/bert-classifier.png\"\n",
        "HTML(f\"<img src='{bert_classificationURL}' width='700' height='400' controls></img>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "7rvAL9dYeBpQ",
        "outputId": "46778546-feb5-4153-a870-908601be7bfe"
      },
      "id": "7rvAL9dYeBpQ",
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src='https://jalammar.github.io/images/bert-classifier.png' width='700' height='400' controls></img>"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Hugging Face Library"
      ],
      "metadata": {
        "id": "6_gOn9R3yFDz"
      },
      "id": "6_gOn9R3yFDz"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenization_URL = \"https://jalammar.github.io/images/distilBERT/bert-distilbert-tokenization-2-token-ids.png\"\n",
        "HTML(f\"<img src='{tokenization_URL}' width='1000' height='600' controls></img>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "xPFADvPCiAUJ",
        "outputId": "11a3f62a-8363-43e3-fd00-5d8d5f445088"
      },
      "id": "xPFADvPCiAUJ",
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src='https://jalammar.github.io/images/distilBERT/bert-distilbert-tokenization-2-token-ids.png' width='1000' height='600' controls></img>"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classification_process_full_URL = \"https://jalammar.github.io/images/distilBERT/bert-model-calssification-output-vector-cls.png\"\n",
        "HTML(f\"<img src='{classification_process_full_URL}' width='1000' height='600' controls></img>\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "FODPWwp4iiMP",
        "outputId": "09b8c4ba-3df6-4ac1-85b1-d170d381f0ed"
      },
      "id": "FODPWwp4iiMP",
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src='https://jalammar.github.io/images/distilBERT/bert-model-calssification-output-vector-cls.png' width='1000' height='600' controls></img>"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3W-m5HUBiqG4"
      },
      "id": "3W-m5HUBiqG4",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "",
      "name": ""
    },
    "language_info": {
      "name": ""
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}