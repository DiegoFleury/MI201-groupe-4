{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc4f5bf-4556-4b77-b199-0dd0939f1234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "from autocorrect import Speller\n",
    "import re\n",
    "import contractions\n",
    "import emoji\n",
    "\n",
    "# Checar esses acentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ded12c4-dec0-4598-84a5-9e233f981594",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntacticEmbeddingPipeline:\n",
    "    def __init__(self, word2vec_model, glove_embeddings, pos_labels):\n",
    "        \"\"\"\n",
    "        Pipeline para gerar representações sintáticas para cada palavra em um texto.\n",
    "\n",
    "        Args:\n",
    "            word2vec_model (gensim.models.Word2Vec): Modelo Word2Vec treinado.\n",
    "            glove_embeddings (dict): Embeddings GloVe carregados.\n",
    "            pos_labels (list): Lista de etiquetas POS para one-hot encoding.\n",
    "        \"\"\"\n",
    "        self.word2vec = word2vec_model\n",
    "        self.glove = glove_embeddings\n",
    "        self.pos_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "        self.pos_encoder.fit(np.array(pos_labels).reshape(-1, 1))\n",
    "        self.nlp = spacy.load(\"en_core_web_lg\")\n",
    "        self.spell = Speller(lang='en')\n",
    "        self.slang_map = {\n",
    "            \"asap\": \"as soon as possible\",\n",
    "            \"idk\": \"i do not know\",\n",
    "            \"lol\": \"laughing out loud\"\n",
    "        }\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Pré-processa o texto inicial.\"\"\"\n",
    "        text = self.spell(text)  # Corrigir erros ortográficos\n",
    "        text = emoji.replace_emoji(text, replace=\"\")  # Remover emojis\n",
    "        text = re.sub(r'[\\n\\t]', ' ', text)  # Corrigir quebras de linha\n",
    "        text = re.sub(r'[\\u0300-\\u036f]', '', text)  # Remover acentos\n",
    "        text = contractions.fix(text)  # Expandir contrações\n",
    "        text = re.sub(r'http\\S+|www\\S+|\\w+\\.\\w{2,3}|#[\\w]+|<[^>]+>', '', text)  # Remover links, URLs, hashtags, XMLs\n",
    "        text = ' '.join([self.slang_map.get(word.lower(), word) for word in text.split()])\n",
    "        return text.strip()\n",
    "\n",
    "    def get_pos_one_hot(self, text):\n",
    "        \"\"\"Gera a codificação one-hot das etiquetas POS do texto pré-processado.\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        pos_tags = [token.pos_ for token in doc]\n",
    "        pos_one_hot = [self.pos_encoder.transform(np.array([pos]).reshape(-1, 1)).toarray()[0] for pos in pos_tags]\n",
    "        return np.array(pos_one_hot)\n",
    "\n",
    "    def preprocess_for_embeddings(self, text):\n",
    "        \"\"\"Aplica lematização e remove pontuações para uso em Word2Vec e GloVe.\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        tokens = [token.lemma_.lower() for token in doc if not token.is_punct and not token.is_space]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def get_word2vec_embedding(self, word):\n",
    "        \"\"\"Obtém o embedding Word2Vec para uma palavra.\"\"\"\n",
    "        return self.word2vec.wv[word] if word in self.word2vec.wv else np.zeros(self.word2vec.vector_size)\n",
    "\n",
    "    def get_glove_embedding(self, word):\n",
    "        \"\"\"Obtém o embedding GloVe para uma palavra.\"\"\"\n",
    "        return self.glove.get(word, np.zeros(len(next(iter(self.glove.values())))))\n",
    "\n",
    "    def process_sentence(self, sentence):\n",
    "        \"\"\"Gera a representação concatenada para cada palavra em uma sentença.\"\"\"\n",
    "        # Pré-processar o texto\n",
    "        preprocessed_text = self.preprocess_text(sentence)\n",
    "\n",
    "        # Codificar POS tags em one-hot\n",
    "        pos_one_hot_embeddings = self.get_pos_one_hot(preprocessed_text)\n",
    "\n",
    "        # Lematizar e preparar para Word2Vec e GloVe\n",
    "        preprocessed_for_embeddings = self.preprocess_for_embeddings(preprocessed_text)\n",
    "        words = preprocessed_for_embeddings.split()\n",
    "\n",
    "        embeddings = []\n",
    "        for i, word in enumerate(words):\n",
    "            pos_one_hot = pos_one_hot_embeddings[i] if i < len(pos_one_hot_embeddings) else np.zeros(len(pos_one_hot_embeddings[0]))\n",
    "            w2v_embedding = self.get_word2vec_embedding(word)\n",
    "            glove_embedding = self.get_glove_embedding(word)\n",
    "\n",
    "            # Concatenar (POS + Word2Vec + GloVe)\n",
    "            word_embedding = np.concatenate([pos_one_hot, w2v_embedding, glove_embedding])\n",
    "            embeddings.append(word_embedding)\n",
    "\n",
    "        return np.array(embeddings)\n",
    "\n",
    "    def process_text(self, text):\n",
    "        \"\"\"Processa um texto completo (várias frases).\"\"\"\n",
    "        sentences = [sent.text for sent in self.nlp(text).sents]\n",
    "        all_embeddings = [self.process_sentence(sentence) for sentence in sentences]\n",
    "        return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2b665a-c4bd-400c-990a-87c3d7fab8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de dados\n",
    "corpus = [\"I love natural language processing.\", \"Word2Vec is great for syntactic embeddings.\"]\n",
    "\n",
    "# Carregar Word2Vec (exemplo: treinar com corpus de exemplo)\n",
    "sentences = [text.split() for text in corpus]\n",
    "word2vec_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Carregar GloVe (exemplo: dicionário fictício)\n",
    "glove_embeddings = {\n",
    "        \"i\": np.random.rand(100),\n",
    "        \"love\": np.random.rand(100),\n",
    "        \"natural\": np.random.rand(100),\n",
    "        \"language\": np.random.rand(100),\n",
    "        \"processing\": np.random.rand(100),\n",
    "        \"asap\": np.random.rand(100),\n",
    "        \"word2vec\": np.random.rand(100),\n",
    "        \"syntactic\": np.random.rand(100),\n",
    "        \"embeddings\": np.random.rand(100),\n",
    "}\n",
    "\n",
    "# Lista de etiquetas POS para one-hot encoding\n",
    "pos_labels = [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\", \"PRON\", \"DET\", \"ADP\", \"CCONJ\", \"NUM\", \"PART\", \"INTJ\"]\n",
    "\n",
    "# Inicializar a pipeline\n",
    "pipeline = SyntacticEmbeddingPipeline(word2vec_model, glove_embeddings, pos_labels)\n",
    "\n",
    "    # Processar um texto\n",
    "for text in corpus:\n",
    "    embeddings = pipeline.process_text(text)\n",
    "    print(f\"Embeddings para: '{text}'\\n{embeddings}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
