{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258d0f36-6b92-4ee4-bd26-28ba3c788651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "import contractions\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from autocorrect import Speller\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fa1c00-e81b-4ca3-9c9d-e0904c8f00ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexicalEmbeddingPipeline:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Pipeline para gerar representa√ß√µes l√©xicas para textos, baseadas em TF-IDF, contagens e outras m√©tricas.\n",
    "        \"\"\"\n",
    "        self.spell = Speller(lang='en')\n",
    "        self.slang_map = {\n",
    "            \"asap\": \"as soon as possible\",\n",
    "            \"idk\": \"i do not know\",\n",
    "            \"lol\": \"laughing out loud\"\n",
    "        }\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        # Vetorizadores\n",
    "        self.tfidf_vectorizer = None\n",
    "        self.count_vectorizer = None\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Pr√©-processa o texto para limpeza e normaliza√ß√£o.\"\"\"\n",
    "        # Corre√ß√£o ortogr√°fica\n",
    "        text = self.spell(text)\n",
    "\n",
    "        # Substituir emojis por descri√ß√µes (demojify)\n",
    "        text = emoji.replace_emoji(text, replace=lambda e: f\" {emoji.demojize(e)} \")\n",
    "\n",
    "        # Corrigir quebras de linha e remover pontua√ß√£o\n",
    "        text = re.sub(r'[\\n\\t]', ' ', text)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "        # Remover acentos\n",
    "        text = re.sub(r'[\\u0300-\\u036f]', '', text)\n",
    "\n",
    "        # Expandir contra√ß√µes\n",
    "        text = contractions.fix(text)\n",
    "\n",
    "        # # Expandir g√≠rias\n",
    "        # text = ' '.join([self.slang_map.get(word.lower(), word) for word in text.split()])\n",
    "\n",
    "        # Remover links, hashtags, XMLs, etc.\n",
    "        text = re.sub(r'http\\S+|www\\S+|\\w+\\.\\w{2,3}|#[\\w]+|<[^>]+>', '', text)\n",
    "\n",
    "        # Remover stop words e aplicar stemming -> ou lemmas ? \n",
    "        tokens = [self.stemmer.stem(word) for word in text.split() if word.lower() not in self.stop_words]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def fit_vectorizers(self, corpus):\n",
    "        \"\"\"Treina os vetorizadores TF-IDF e Count Vectorizer no corpus.\"\"\"\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "        self.count_vectorizer = CountVectorizer(max_features=5000)\n",
    "\n",
    "        # Ajustar vetorizadores no corpus\n",
    "        self.tfidf_vectorizer.fit(corpus)\n",
    "        self.count_vectorizer.fit(corpus)\n",
    "\n",
    "    def generate_word_embeddings(self, text):\n",
    "        \"\"\"Gera embeddings para cada palavra no texto pr√©-processado.\"\"\"\n",
    "        # Pr√©-processar o texto -> j√° foi feito antes, mas tem q lembrar q isso vai ser usado tbm depois pra teste !!!\n",
    "        tokens = self.preprocess_text(text)\n",
    "\n",
    "        # Obter valores TF-IDF e Count para cada token\n",
    "        tfidf_values = self.tfidf_vectorizer.transform([\" \".join(tokens)]).toarray()[0]\n",
    "        count_values = self.count_vectorizer.transform([\" \".join(tokens)]).toarray()[0]\n",
    "\n",
    "        token_embeddings = []\n",
    "        for token in tokens:\n",
    "            tfidf_value = tfidf_values[self.tfidf_vectorizer.vocabulary_.get(token, 0)]\n",
    "            count_value = count_values[self.count_vectorizer.vocabulary_.get(token, 0)]\n",
    "\n",
    "            # Concatena√ß√£o de valores (sugest√µes adicionais abaixo)\n",
    "            embedding = np.array([tfidf_value, count_value])\n",
    "            token_embeddings.append(embedding)\n",
    "\n",
    "        return tokens, np.array(token_embeddings)\n",
    "\n",
    "    def process_corpus(self, corpus):\n",
    "        \"\"\"Processa o corpus completo, aplicando limpeza e gerando embeddings para cada palavra.\"\"\"\n",
    "        processed_corpus = [\" \".join(self.preprocess_text(text)) for text in corpus]\n",
    "\n",
    "        # Treinar vetorizadores no corpus completo\n",
    "        self.fit_vectorizers(processed_corpus)\n",
    "\n",
    "        # Gerar embeddings por texto\n",
    "        results = []\n",
    "        for text in corpus:\n",
    "            tokens, embeddings = self.generate_word_embeddings(text)\n",
    "            results.append({\"tokens\": tokens, \"embeddings\": embeddings})\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226ed7fe-7459-47ff-9fa8-03e3bd00924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de corpus\n",
    "corpus = [\n",
    "        \"I love natural language processing! üòä\", \n",
    "        \"ASAP, I will learn machine learning techniques. #AI\",\n",
    "        \"TF-IDF and Bag-of-Words are great for lexical analysis.\"\n",
    "]\n",
    "\n",
    "# Inicializar pipeline\n",
    "pipeline = LexicalEmbeddingPipeline()\n",
    "\n",
    "# Processar corpus\n",
    "results = pipeline.process_corpus(corpus)\n",
    "\n",
    "# Exibir resultados\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Texto {i+1} Tokens: {result['tokens']}\")\n",
    "    print(f\"Texto {i+1} Embeddings:\\n{result['embeddings']}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
